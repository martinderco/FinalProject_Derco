{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import unidecode\n",
    "import csv\n",
    "import threading\n",
    "from selenium import webdriver\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Path to the .exe file that starts the Selenium plugin that starts chrome. As the pages I am about to scrape are javascript-based, with a help of Google, I find out it is more convenient (necessary) to go with selenium in order to view the page without refreshing and to be able to scrape it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'chromedriver.exe'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, some of the websites (iFortuna, synottip) I am about to scrape display many matches and need to be scrolled down to the bottom of the page in order to scrape all of the matches - again, I used a help of Google to tackle this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scroll(driver):\n",
    "    # First, we set pause time\n",
    "    SCROLL_PAUSE_TIME = 0.5\n",
    "\n",
    "    # Second, we get scroll height\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        # then, we scroll down to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        # Wait to load page using predefined pause time\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "        # Calculate new scroll height and compare with last scroll height, if equal - stop, if not - keep going\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later on I found out, that synottip uses different way of scroller, so I used the help of Google again to find a way how to scroll down an inner element of a page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scroll_synottip(driver):\n",
    "    SCROLL_PAUSE_TIME = 1\n",
    "    content = driver.find_element_by_class_name('content-container')\n",
    "    content = content.find_element_by_class_name('simplebar-scroll-content')\n",
    "\n",
    "    # Get scroll height - scrolling element found from inspected page\n",
    "    rows = driver.find_elements_by_xpath(\n",
    "        '//div[@data-test-role=\"event-list__item\"]') \n",
    "\n",
    "    while True:\n",
    "        # Scroll down to bottom\n",
    "        driver.execute_script(\"arguments[0].scrollTop = arguments[0].scrollHeight\", content)\n",
    "\n",
    "        # Wait to load page\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_rows = driver.find_elements_by_xpath(\n",
    "            '//div[@data-test-role=\"event-list__item\"]')  \n",
    "\n",
    "        if new_rows == rows:\n",
    "            break\n",
    "        rows = new_rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are finally ready to start scraping individual sports from individual pages. I decided to choose tenis and voleyball, as these two sports have (almost) only win-lose bets and thus it will be easier to both scrape and evaluate the data. The codes are basically identical.\n",
    "\n",
    "With a help of Google I added threads to be able to run all the scrapers at the same time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_tenis():\n",
    "    #first, we define threads that opens individual webpages with corresponding url (sport-relevant)\n",
    "    #arguments scrape_sport_* used later on to find relevant .csv file and url for scraping correct site\n",
    "    t1 = threading.Thread(target=scrape_sport_tipsport,\n",
    "                          args=(\"tenis\", \"https://www.tipsport.cz/kurzy/tenis-43?limit=500\",))\n",
    "    t2 = threading.Thread(target=scrape_sport_ifortuna,\n",
    "                          args=(\"tenis\", \"https://www.ifortuna.cz/sazeni/tenis\",))\n",
    "    t3 = threading.Thread(target=scrape_sport_synottip,\n",
    "                          args=(\"tenis\", \"https://sport.synottip.cz/#/zapasy/19?categoryId=19\",))\n",
    "    t4 = threading.Thread(target=scrape_sport_chance,\n",
    "                          args=(\"tenis\", \"https://www.chance.cz/kurzy/tenis-43?limit=500\",))\n",
    "    t1.start()\n",
    "    t2.start()\n",
    "    t3.start()\n",
    "    t4.start()\n",
    "    t1.join()\n",
    "    t2.join()\n",
    "    t3.join()\n",
    "    t4.join()\n",
    "\n",
    "    \n",
    "def scrape_voleyball():\n",
    "    t1 = threading.Thread(target=scrape_sport_tipsport,\n",
    "                          args=(\"voleyball\", \"https://www.tipsport.cz/kurzy/volejbal-47?limit=500\",))\n",
    "    t2 = threading.Thread(target=scrape_sport_ifortuna,\n",
    "                          args=(\"voleyball\", \"https://www.ifortuna.cz/sazeni/volejbal\",))\n",
    "    t3 = threading.Thread(target=scrape_sport_synottip,\n",
    "                          args=(\"voleyball\", \"https://sport.synottip.cz/#/zapasy/23?categoryId=23\",))\n",
    "    t4 = threading.Thread(target=scrape_sport_chance,\n",
    "                          args=(\"voleyball\", \"https://www.chance.cz/kurzy/volejbal-47?limit=500\",))\n",
    "    t1.start()\n",
    "    t2.start()\n",
    "    t3.start()\n",
    "    t4.start()\n",
    "    t1.join()\n",
    "    t2.join()\n",
    "    t3.join()\n",
    "    t4.join()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the betting company webpage has its own scraper which are very similar in general. Two of them use the aformentioned scrollers (synottip and ifortuna) and all of them differ in having unique page structure. As I have mentioned before tenis and voleyball have mostly only win-lose options which made the scraping slightly easier (data will be cleaned of different observations in the next section) and thus through inspecting individual pages I built individual scrapers.\n",
    "For future analysis, it is important to save date from all the sources in the same format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_sport_chance(sport, url):\n",
    "    # Selenium driver\n",
    "    driver = webdriver.Chrome(path)\n",
    "    #empty field with matches that I will save to csv when filled with data\n",
    "    matches = []\n",
    "    #driver gets url where to scrape from\n",
    "    driver.get(url)\n",
    "    #3s break to ensure Chrome had enough time to start\n",
    "    time.sleep(3)\n",
    "    #fid rows with matches\n",
    "    rows = driver.find_elements_by_class_name('o-matchRow__main')\n",
    "\n",
    "    #And for each of these rows:\n",
    "    for row in rows:\n",
    "        #looking for match Name and date (playtime)\n",
    "        left = row.find_element_by_class_name('o-matchRow__leftSide')\n",
    "        #unidecode used everytime when name is stored in order to remove diacritics\n",
    "        name = unidecode.unidecode(left.find_element_by_class_name('o-matchRow__matchName').text)\n",
    "        dateSpans = left.find_element_by_class_name('o-matchRow__dateClosed').find_elements_by_xpath('span')\n",
    "        #in case date and playtime are stored in different elements we need to combine these together and store the information\n",
    "        #in specific format. In case playtime is not available only date is extracted - does not matter for future matching\n",
    "        if len(dateSpans) > 1:\n",
    "            date = left.find_element_by_class_name('o-matchRow__dateClosed').find_elements_by_xpath('span')[0].text\n",
    "            date = date + ' ' + \\\n",
    "                   left.find_element_by_class_name('o-matchRow__dateClosed').find_elements_by_xpath('span')[1].text\n",
    "            date = datetime.strptime(date, '%d.%m.%Y %H:%M')\n",
    "        else:\n",
    "            date = left.find_element_by_class_name('o-matchRow__dateClosed').find_elements_by_xpath('span')[0]\n",
    "            date = datetime.strptime(date, '%d.%m.%Y')\n",
    "        #in the right element we are scraping for the rates\n",
    "        right = row.find_element_by_class_name('o-matchRow__rightSide')\n",
    "        rates = right.find_element_by_class_name('o-matchRow__rightSideInner').find_element_by_class_name(\n",
    "            'm-matchRowOdds').find_elements_by_class_name('btnRate')\n",
    "        #we predefine win-draw-loss (home-draw-away) as zeros (including draw for possible future use with different sports\n",
    "        #including this option) in order to detect possible unsuitable observations missing either home or away values.\n",
    "        home = 0\n",
    "        away = 0\n",
    "        draw = 0\n",
    "        if len(rates) == 2:\n",
    "            home = rates[0].text\n",
    "            away = rates[1].text\n",
    "        if len(rates) == 3:\n",
    "            home = rates[0].text\n",
    "            draw = rates[1].text\n",
    "            away = rates[2].text\n",
    "        #if home or away was not found, dont append the observation\n",
    "        if home != 0 and away != 0:\n",
    "            matches.append([name, date, home, draw, away])\n",
    "    #fill .csv file with the scraped matches (rewrites the existing values in provided datafile with fresh ones)\n",
    "    with open('chance/' + sport + '.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "        write = csv.writer(f)\n",
    "        write.writerow(['name', 'date', 'home', 'draw', 'away'])\n",
    "        write.writerows(matches)\n",
    "    #close chrome driver\n",
    "    driver.quit()\n",
    "\n",
    "#same as above, except for using scroller\n",
    "def scrape_sport_synottip(sport, url):\n",
    "    driver = webdriver.Chrome(path)\n",
    "    matches = []\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    scroll_synottip(driver)\n",
    "    rows = driver.find_elements_by_xpath(\n",
    "        '//div[@data-test-role=\"event-list__item\"]')  \n",
    "\n",
    "    for row in rows:\n",
    "        odds = row.find_elements_by_class_name('rate')\n",
    "        name = unidecode.unidecode(row.find_element_by_class_name('match-label').text)\n",
    "        if len(odds) > 1:\n",
    "            try:\n",
    "                #here we need to edit existing strings in order to reformat as float due to different float formatting\n",
    "                date = row.find_element_by_class_name('v-center').text.replace('\\n', ' ')\n",
    "                date = datetime.strptime(date, '%d.%m.%y %H:%M')\n",
    "                odds1 = odds[0].text\n",
    "                odds1 = float(odds1.replace(\",\", \".\"))\n",
    "                odds2 = odds[1].text\n",
    "                odds2 = float(odds2.replace(\",\", \".\"))\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            home = 0\n",
    "            away = 0\n",
    "            draw = 0\n",
    "            if len(odds) == 2:\n",
    "                home = odds1\n",
    "                away = odds2\n",
    "            if len(odds) == 3:\n",
    "                home = odds1\n",
    "                draw = odds2\n",
    "                odds3 = odds[2].text\n",
    "                odds3 = float(odds3.replace(\",\", \".\"))\n",
    "                away = odds3\n",
    "            if home != 0 and away != 0:\n",
    "                matches.append([name, date, home, draw, away])\n",
    "\n",
    "    with open('synottip/' + sport + '.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "        write = csv.writer(f)\n",
    "        write.writerow(['name', 'date', 'home', 'draw', 'away'])\n",
    "        write.writerows(matches)\n",
    "    driver.quit()\n",
    "\n",
    "#same as above, except for using different scroller\n",
    "def scrape_sport_ifortuna(sport, url):\n",
    "    driver = webdriver.Chrome(path)\n",
    "    matches = []\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    scroll(driver)\n",
    "    rows = driver.find_elements_by_xpath(\"//table/tbody/tr\")\n",
    "    for row in rows:\n",
    "        try:\n",
    "            home = 0\n",
    "            draw = 0\n",
    "            away = 0\n",
    "            name = row.find_element_by_class_name('col-title').text\n",
    "            date = datetime.fromtimestamp(\n",
    "                float(row.find_element_by_class_name('col-date').get_attribute(\"data-value\")) / 1000)\n",
    "            rates = row.find_elements_by_class_name('col-odds')\n",
    "            if len(rates) == 2:\n",
    "                home = rates[0].text\n",
    "                away = rates[1].text\n",
    "            if len(rates) >= 3:\n",
    "                home = rates[0].text\n",
    "                draw = rates[1].text\n",
    "                away = rates[2].text\n",
    "            matches.append([name, date, home, draw, away])\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    if len(matches) > 0:\n",
    "        with open('ifortuna/' + sport + '.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "            write = csv.writer(f)\n",
    "            write.writerow(['name', 'date', 'home', 'draw', 'away'])\n",
    "            write.writerows(matches)\n",
    "    driver.quit()\n",
    "\n",
    "#identical to chance (much likely the same owner - same webpage, same rates (we find later on))\n",
    "def scrape_sport_tipsport(sport, url):\n",
    "    driver = webdriver.Chrome(path)\n",
    "    matches = []\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    rows = driver.find_elements_by_class_name('o-matchRow__main')\n",
    "\n",
    "    for row in rows:\n",
    "        left = row.find_element_by_class_name('o-matchRow__leftSide')\n",
    "        name = unidecode.unidecode(left.find_element_by_class_name('o-matchRow__matchName').text)\n",
    "        dateSpans = left.find_element_by_class_name('o-matchRow__dateClosed').find_elements_by_xpath('span')\n",
    "        if len(dateSpans) > 1:\n",
    "            date = left.find_element_by_class_name('o-matchRow__dateClosed').find_elements_by_xpath('span')[0].text\n",
    "            date = date + ' ' + \\\n",
    "                   left.find_element_by_class_name('o-matchRow__dateClosed').find_elements_by_xpath('span')[1].text\n",
    "            date = datetime.strptime(date, '%d.%m.%Y %H:%M')\n",
    "        else:\n",
    "            date = left.find_element_by_class_name('o-matchRow__dateClosed').find_elements_by_xpath('span')[0]\n",
    "            date = datetime.strptime(date, '%d.%m.%Y')\n",
    "        right = row.find_element_by_class_name('o-matchRow__rightSide')\n",
    "        rates = right.find_element_by_class_name('o-matchRow__rightSideInner').find_element_by_class_name(\n",
    "            'm-matchRowOdds').find_elements_by_class_name('btnRate')\n",
    "        home = 0\n",
    "        away = 0\n",
    "        draw = 0\n",
    "        if len(rates) == 2:\n",
    "            home = rates[0].text\n",
    "            away = rates[1].text\n",
    "        if len(rates) == 3:\n",
    "            home = rates[0].text\n",
    "            draw = rates[1].text\n",
    "            away = rates[2].text\n",
    "\n",
    "        if home != 0 and away != 0:\n",
    "            matches.append([name, date, home, draw, away])\n",
    "\n",
    "    with open('tipsport/' + sport + '.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "        write = csv.writer(f)\n",
    "        write.writerow(['name', 'date', 'home', 'draw', 'away'])\n",
    "        write.writerows(matches)\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run scrapers for both sports for each of the webpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_voleyball()\n",
    "scrape_tenis()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
