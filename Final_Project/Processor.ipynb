{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as we scraped data from 4 different webpages, we try to match same matches as each of the webpages names them slightly differently. \n",
    "First, we find the betting office with the highest amount of available matches.\n",
    "Then, we find unique dates and playtimes.\n",
    "Next, we try to match the matches by playtime across our 4 different datasets. \n",
    "Now, we try to ensure we matched the identical matches by looking for matches starting with the same 1st word among the ones with the same playtime.\n",
    "If we get 1 exact match, we are happy. \n",
    "We save these matches into csv_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_exchange_profit_for_sport(sport):\n",
    "    #empty array that will be used for sport_final dataset\n",
    "    rowsToAppend = []\n",
    "    rowsToAppend.append([\"Match_name\", \"Date\", \"Chance\", \"Ifortuna\", \"Synot\", \"Tipsport\"])\n",
    "    chance = pd.read_csv('chance/' + sport + '.csv')\n",
    "    ifortuna = pd.read_csv('ifortuna/' + sport + '.csv')\n",
    "    synottip = pd.read_csv('synottip/' + sport + '.csv')\n",
    "    tipsport = pd.read_csv('tipsport/' + sport + '.csv')\n",
    "    chance = chance.sort_values(by=['date'])\n",
    "    ifortuna = ifortuna.sort_values(by=['date'])\n",
    "    synottip = synottip.sort_values(by=['date'])\n",
    "    tipsport = tipsport.sort_values(by=['date'])\n",
    "#setting max to -1 as we want to be sure this value will be overwritten by at least 0 matches at a specific playtime\n",
    "#we need to set \"largest\" value even if there are no matches for the code to work\n",
    "max = -1\n",
    "    if (chance.size > max):\n",
    "        largest = chance\n",
    "        max = chance.size\n",
    "    if (ifortuna.size > max):\n",
    "        largest = ifortuna\n",
    "        max = ifortuna.size\n",
    "    if (synottip.size > max):\n",
    "        largest = synottip\n",
    "        max = synottip.size\n",
    "    if (tipsport.size > max):\n",
    "        largest = tipsport\n",
    "    dates = largest['date'].unique()\n",
    "    #create new dataframe for each playtime for each webpage\n",
    "    for date in dates:\n",
    "        matches_for_date_chance = chance.loc[chance['date'] == date]\n",
    "        matches_for_date_ifortuna = ifortuna.loc[ifortuna['date'] == date]\n",
    "        matches_for_date_synottip = synottip.loc[synottip['date'] == date]\n",
    "        matches_for_date_tipsport = tipsport.loc[tipsport['date'] == date]\n",
    "        #find the webpage with highest no. of observations for a specific playtime\n",
    "        max = -1\n",
    "        if (matches_for_date_chance.size > max):\n",
    "            largest = matches_for_date_chance\n",
    "            max = matches_for_date_chance.size\n",
    "        if (matches_for_date_ifortuna.size > max):\n",
    "            largest = matches_for_date_ifortuna\n",
    "            max = matches_for_date_ifortuna.size\n",
    "        if (matches_for_date_synottip.size > max):\n",
    "            largest = matches_for_date_synottip\n",
    "            max = matches_for_date_synottip.size\n",
    "        if (matches_for_date_tipsport.size > max):\n",
    "            largest = matches_for_date_tipsport\n",
    "        names = largest['name'].unique()\n",
    "        #second forloop to find the exact match by 1st word in the matches' name\n",
    "        for name in names:\n",
    "            try:\n",
    "                #create blank array and add name and date\n",
    "                row = []\n",
    "                row.append(name)\n",
    "                row.append(date)\n",
    "                # here we try to clean the names of the matches of special symbols, that could complicate the matching process\n",
    "                # parsed = divide name into single words and pick the 1st one \n",
    "                parsed = str(name).replace(',', ' ').replace('/', ' ').split(' ')\n",
    "                identifier = parsed[0]\n",
    "                #for each url we try to find this match with this specific identifier\n",
    "                matches_for_date_chance_select = matches_for_date_chance.loc[\n",
    "                    matches_for_date_chance['name'].str.contains(identifier)]\n",
    "                #when we find exactly one match, we are happy, and save it as a list\n",
    "                if (len(matches_for_date_chance_select.index) == 1):\n",
    "                    values = matches_for_date_chance_select.iloc[0].values.tolist()\n",
    "                    #crucial formula = this formula evaluates the exchange profit/margin of individual booking companies \n",
    "                    #as we mentioned before, there still may be observations of bets, that do not bet on win-lose\n",
    "                    #such are bets on final winner etc. therefore, we rule out these bets with following condition\n",
    "                    #as these bets will have rates that fail to comply with our ratio (will be below 1)\n",
    "                    #we save these ratios for each match for each company if available, otherwise append zero\n",
    "                    exchangeProfit = 1/float(values[2]) + 1/float(values[4])\n",
    "                    if (exchangeProfit > 1):\n",
    "                        row.append(exchangeProfit)\n",
    "                    else:\n",
    "                        row.append(0)\n",
    "                else:\n",
    "                    row.append(0)\n",
    "                #repeat for all the betting offices\n",
    "                matches_for_date_ifortuna_select = matches_for_date_ifortuna.loc[\n",
    "                    matches_for_date_ifortuna['name'].str.contains(identifier)]\n",
    "                if (len(matches_for_date_ifortuna_select.index) == 1):\n",
    "                    values = matches_for_date_ifortuna_select.iloc[0].values.tolist()\n",
    "                    exchangeProfit = 1 / float(values[2]) + 1 / float(values[4])\n",
    "                    if (exchangeProfit > 1):\n",
    "                        row.append(exchangeProfit)\n",
    "                    else:\n",
    "                        row.append(0)\n",
    "                else:\n",
    "                    row.append(0)\n",
    "\n",
    "                matches_for_date_synottip_select = matches_for_date_synottip.loc[\n",
    "                    matches_for_date_synottip['name'].str.contains(identifier)]\n",
    "                if (len(matches_for_date_synottip_select.index) == 1):\n",
    "                    values = matches_for_date_synottip_select.iloc[0].values.tolist()\n",
    "                    exchangeProfit = 1 / float(values[2]) + 1 / float(values[4])\n",
    "                    if (exchangeProfit > 1):\n",
    "                        row.append(exchangeProfit)\n",
    "                    else:\n",
    "                        row.append(0)\n",
    "                else:\n",
    "                    row.append(0)\n",
    "\n",
    "                matches_for_date_tipsport_select = matches_for_date_tipsport.loc[\n",
    "                    matches_for_date_tipsport['name'].str.contains(identifier)]\n",
    "                if (len(matches_for_date_tipsport_select.index) == 1):\n",
    "                    values = matches_for_date_tipsport_select.iloc[0].values.tolist()\n",
    "                    exchangeProfit = 1 / float(values[2]) + 1 / float(values[4])\n",
    "                    if (exchangeProfit > 1):\n",
    "                        row.append(exchangeProfit)\n",
    "                    else:\n",
    "                        row.append(0)\n",
    "                else:\n",
    "                    row.append(0)\n",
    "                \n",
    "                #observations with incorrect/zero values shall not be appended\n",
    "                if(row[2] == 0 and row[3] == 0 and row[4] == 0 and row[5] == 0):\n",
    "                    continue\n",
    "                else:\n",
    "                    rowsToAppend.append(row)\n",
    "            except:\n",
    "                print()\n",
    "    df = pd.DataFrame(rowsToAppend)\n",
    "    df.to_csv(sport + '_final.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run individual analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "analyze_exchange_profit_for_sport('tenis')\n",
    "analyze_exchange_profit_for_sport('voleyball')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
